{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Shehroz Sohail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://cdn.wccftech.com/wp-content/uploads/2021/02/twitter-for-iOS.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "+ We were hired by twitter to come up with a default sentiment analysis model which can be integrated into their API and will make analysis sentiment throug hthe twitter API much much easier. For the purpose of this research we were given access to their API network and selected to analyze the current situation of Russia-Ukraine. \n",
    "+ Extracted 50,000 tweets with the Russia hashtag and 50,000 tweets with the Ukraine hashtag. This ensures that both of the sides are represented and no unnecessary bias is introduced into the model.\n",
    "\n",
    "### Analysis.\n",
    "+ For this purpose we will be comparing two techniques of sentiment analysis. The first one is the vader sentiment analysis in the NLTK library. The vader sentiment has a dictionary of positive words and negative words in it and simply calculates the sentiment by counting how many time a positive or a negative word appears in the snetence. Then it calcuates probabilities and hence assigns labels.\n",
    "+ The other technique is the neural network whcih unlike the previous approach which considers each word to be independant from the context of the sentence, this model considers the previous word while assigning labels to the sentence.\n",
    "\n",
    "### Approach.\n",
    "+ After extracting tweets using the twitter APi. we will first split our dataset into train, validate and test set.\n",
    "+ we will apply vader sentiment on all of the given datasets and save them for later use. \n",
    "+ The train and validate set will be used to create our model (supervised learning using labels from the vader sentiment)\n",
    "+ I will then label the test set physically and use it as  a basis to check which model performs better.\n",
    "+ Both the vader sentiment and model's accuracy scores will be calculated on the physically labeled test set to see which one performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "# import tweepy\n",
    "# from tweepy import *\n",
    "import datetime\n",
    "\n",
    "#NLTK\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorboard.plugins import projector\n",
    "import IPython\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Necessary Keys/Secrets to work with twitter API.\n",
    "+ All of the codes have been changed and will not work in a new notebook unless new codes are inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'yALdWFxZ0DVDSDGYczMzakfVn'\n",
    "consumer_secret = '4YgHvQz9rDjMv5Btq9b9ygimkkKZb0jtLFwoYSg5eqjMaSf0Vo'\n",
    "access_key= '903728159010508801-Fg8gJnZ6vLzRkYUWJgNf2L4EpJcXgIv'\n",
    "access_secret = 'DNMq4P2BRSsqYszuwlL2iaoGVmujF5nUZoOMa8GKvUEAV'\n",
    "Bearer_token='AAAAAAAAAAAAAAAAAAAAAHI8aQEAAAAA8QCbMBdiz4xpnNO3XAu2ikD77SI%3D7kzqydWWB7iWtMhqZpMblJg3ya81hqe9GjXh7KDh2xpFxTuzzG'\n",
    "\n",
    "\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    " \n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "yesterday= today - datetime.timedelta(days=300)\n",
    "keywords=\"#Russia since:\" + str(yesterday)+ \" until:\" + str(today)\n",
    "# keywords = 'russia'\n",
    "limit=50000\n",
    "\n",
    "tweets = tweepy.Cursor(api.search_tweets,since_id=\"2022-01-01\", q=keywords, count=300, tweet_mode='extended',lang=\"en\").items(limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A tweepy Cursor is an item iterator that contains JSON files at each iteration (till the limit is reached)\n",
    "+ Displaying the first json output and extracting information that is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'created_at': 'Sat Mar 19 23:59:58 +0000 2022',\n",
       " 'id': 1505333290990940160,\n",
       " 'id_str': '1505333290990940160',\n",
       " 'full_text': 'RT @NewUkraineNews: Female #Ukrainian soldier with a #Javelin anti tank missile launcher, near #Kiev.\\n#Ukraine #UkraineRussiaWar #Russia #P‚Ä¶',\n",
       " 'truncated': False,\n",
       " 'display_text_range': [0, 140],\n",
       " 'entities': {'hashtags': [{'text': 'Ukrainian', 'indices': [27, 37]},\n",
       "   {'text': 'Javelin', 'indices': [53, 61]},\n",
       "   {'text': 'Kiev', 'indices': [95, 100]},\n",
       "   {'text': 'Ukraine', 'indices': [102, 110]},\n",
       "   {'text': 'UkraineRussiaWar', 'indices': [111, 128]},\n",
       "   {'text': 'Russia', 'indices': [129, 136]}],\n",
       "  'symbols': [],\n",
       "  'user_mentions': [{'screen_name': 'NewUkraineNews',\n",
       "    'name': 'Ukraine News Now',\n",
       "    'id': 1496723055506169856,\n",
       "    'id_str': '1496723055506169856',\n",
       "    'indices': [3, 18]}],\n",
       "  'urls': []},\n",
       " 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'},\n",
       " 'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>',\n",
       " 'in_reply_to_status_id': None,\n",
       " 'in_reply_to_status_id_str': None,\n",
       " 'in_reply_to_user_id': None,\n",
       " 'in_reply_to_user_id_str': None,\n",
       " 'in_reply_to_screen_name': None,\n",
       " 'user': {'id': 1382404195848916993,\n",
       "  'id_str': '1382404195848916993',\n",
       "  'name': 'üåèThe world is full of loveüßúüèª\\u200d‚ôÄÔ∏è',\n",
       "  'screen_name': 'yknkXUxwdggMPBs',\n",
       "  'location': '',\n",
       "  'description': '#Womenwarriors around the world are strong, beautiful, and smart. Know your enemy&know yourself, and you will fight a hundred battles ‰∏ñÁïå‰∏≠„ÅÆÂ•≥ÊÄßÂÖµÂ£´„ÅØÁæé„Åó„ÅèÂº∑„Åè„ÄÅË≥¢„ÅÑ\\u3000Â∑±„Å®Êïµ„ÇíÁü•„Çå‚ùóÔ∏è',\n",
       "  'url': None,\n",
       "  'entities': {'description': {'urls': []}},\n",
       "  'protected': False,\n",
       "  'followers_count': 713,\n",
       "  'friends_count': 2935,\n",
       "  'listed_count': 6,\n",
       "  'created_at': 'Wed Apr 14 18:43:51 +0000 2021',\n",
       "  'favourites_count': 6786,\n",
       "  'utc_offset': None,\n",
       "  'time_zone': None,\n",
       "  'geo_enabled': False,\n",
       "  'verified': False,\n",
       "  'statuses_count': 2734,\n",
       "  'lang': None,\n",
       "  'contributors_enabled': False,\n",
       "  'is_translator': False,\n",
       "  'is_translation_enabled': False,\n",
       "  'profile_background_color': 'F5F8FA',\n",
       "  'profile_background_image_url': None,\n",
       "  'profile_background_image_url_https': None,\n",
       "  'profile_background_tile': False,\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/1382630083379175425/v7RzPcal_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1382630083379175425/v7RzPcal_normal.jpg',\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1382404195848916993/1618456600',\n",
       "  'profile_link_color': '1DA1F2',\n",
       "  'profile_sidebar_border_color': 'C0DEED',\n",
       "  'profile_sidebar_fill_color': 'DDEEF6',\n",
       "  'profile_text_color': '333333',\n",
       "  'profile_use_background_image': True,\n",
       "  'has_extended_profile': True,\n",
       "  'default_profile': True,\n",
       "  'default_profile_image': False,\n",
       "  'following': False,\n",
       "  'follow_request_sent': False,\n",
       "  'notifications': False,\n",
       "  'translator_type': 'none',\n",
       "  'withheld_in_countries': []},\n",
       " 'geo': None,\n",
       " 'coordinates': None,\n",
       " 'place': None,\n",
       " 'contributors': None,\n",
       " 'retweeted_status': {'created_at': 'Wed Mar 02 06:49:49 +0000 2022',\n",
       "  'id': 1498913449635500037,\n",
       "  'id_str': '1498913449635500037',\n",
       "  'full_text': 'Female #Ukrainian soldier with a #Javelin anti tank missile launcher, near #Kiev.\\n#Ukraine #UkraineRussiaWar #Russia #Putin https://t.co/dZpXR8yD96',\n",
       "  'truncated': False,\n",
       "  'display_text_range': [0, 123],\n",
       "  'entities': {'hashtags': [{'text': 'Ukrainian', 'indices': [7, 17]},\n",
       "    {'text': 'Javelin', 'indices': [33, 41]},\n",
       "    {'text': 'Kiev', 'indices': [75, 80]},\n",
       "    {'text': 'Ukraine', 'indices': [82, 90]},\n",
       "    {'text': 'UkraineRussiaWar', 'indices': [91, 108]},\n",
       "    {'text': 'Russia', 'indices': [109, 116]},\n",
       "    {'text': 'Putin', 'indices': [117, 123]}],\n",
       "   'symbols': [],\n",
       "   'user_mentions': [],\n",
       "   'urls': [],\n",
       "   'media': [{'id': 1498913421797969921,\n",
       "     'id_str': '1498913421797969921',\n",
       "     'indices': [124, 147],\n",
       "     'media_url': 'http://pbs.twimg.com/media/FM010MdVUAEy4Wb.jpg',\n",
       "     'media_url_https': 'https://pbs.twimg.com/media/FM010MdVUAEy4Wb.jpg',\n",
       "     'url': 'https://t.co/dZpXR8yD96',\n",
       "     'display_url': 'pic.twitter.com/dZpXR8yD96',\n",
       "     'expanded_url': 'https://twitter.com/NewUkraineNews/status/1498913449635500037/photo/1',\n",
       "     'type': 'photo',\n",
       "     'sizes': {'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "      'large': {'w': 720, 'h': 900, 'resize': 'fit'},\n",
       "      'small': {'w': 544, 'h': 680, 'resize': 'fit'},\n",
       "      'medium': {'w': 720, 'h': 900, 'resize': 'fit'}}}]},\n",
       "  'extended_entities': {'media': [{'id': 1498913421797969921,\n",
       "     'id_str': '1498913421797969921',\n",
       "     'indices': [124, 147],\n",
       "     'media_url': 'http://pbs.twimg.com/media/FM010MdVUAEy4Wb.jpg',\n",
       "     'media_url_https': 'https://pbs.twimg.com/media/FM010MdVUAEy4Wb.jpg',\n",
       "     'url': 'https://t.co/dZpXR8yD96',\n",
       "     'display_url': 'pic.twitter.com/dZpXR8yD96',\n",
       "     'expanded_url': 'https://twitter.com/NewUkraineNews/status/1498913449635500037/photo/1',\n",
       "     'type': 'photo',\n",
       "     'sizes': {'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "      'large': {'w': 720, 'h': 900, 'resize': 'fit'},\n",
       "      'small': {'w': 544, 'h': 680, 'resize': 'fit'},\n",
       "      'medium': {'w': 720, 'h': 900, 'resize': 'fit'}}}]},\n",
       "  'metadata': {'iso_language_code': 'en', 'result_type': 'recent'},\n",
       "  'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',\n",
       "  'in_reply_to_status_id': None,\n",
       "  'in_reply_to_status_id_str': None,\n",
       "  'in_reply_to_user_id': None,\n",
       "  'in_reply_to_user_id_str': None,\n",
       "  'in_reply_to_screen_name': None,\n",
       "  'user': {'id': 1496723055506169856,\n",
       "   'id_str': '1496723055506169856',\n",
       "   'name': 'Ukraine News Now',\n",
       "   'screen_name': 'NewUkraineNews',\n",
       "   'location': '',\n",
       "   'description': 'Newest news on Ukraine',\n",
       "   'url': None,\n",
       "   'entities': {'description': {'urls': []}},\n",
       "   'protected': False,\n",
       "   'followers_count': 10359,\n",
       "   'friends_count': 0,\n",
       "   'listed_count': 95,\n",
       "   'created_at': 'Thu Feb 24 05:46:15 +0000 2022',\n",
       "   'favourites_count': 38,\n",
       "   'utc_offset': None,\n",
       "   'time_zone': None,\n",
       "   'geo_enabled': False,\n",
       "   'verified': False,\n",
       "   'statuses_count': 428,\n",
       "   'lang': None,\n",
       "   'contributors_enabled': False,\n",
       "   'is_translator': False,\n",
       "   'is_translation_enabled': False,\n",
       "   'profile_background_color': 'F5F8FA',\n",
       "   'profile_background_image_url': None,\n",
       "   'profile_background_image_url_https': None,\n",
       "   'profile_background_tile': False,\n",
       "   'profile_image_url': 'http://pbs.twimg.com/profile_images/1499959584911331332/TFnQSr01_normal.png',\n",
       "   'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1499959584911331332/TFnQSr01_normal.png',\n",
       "   'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1496723055506169856/1645764421',\n",
       "   'profile_link_color': '1DA1F2',\n",
       "   'profile_sidebar_border_color': 'C0DEED',\n",
       "   'profile_sidebar_fill_color': 'DDEEF6',\n",
       "   'profile_text_color': '333333',\n",
       "   'profile_use_background_image': True,\n",
       "   'has_extended_profile': True,\n",
       "   'default_profile': True,\n",
       "   'default_profile_image': False,\n",
       "   'following': False,\n",
       "   'follow_request_sent': False,\n",
       "   'notifications': False,\n",
       "   'translator_type': 'none',\n",
       "   'withheld_in_countries': []},\n",
       "  'geo': None,\n",
       "  'coordinates': None,\n",
       "  'place': None,\n",
       "  'contributors': None,\n",
       "  'is_quote_status': False,\n",
       "  'retweet_count': 55,\n",
       "  'favorite_count': 280,\n",
       "  'favorited': False,\n",
       "  'retweeted': False,\n",
       "  'possibly_sensitive': False,\n",
       "  'lang': 'en'},\n",
       " 'is_quote_status': False,\n",
       " 'retweet_count': 55,\n",
       " 'favorite_count': 0,\n",
       " 'favorited': False,\n",
       " 'retweeted': False,\n",
       " 'lang': 'en'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tweet in tweets:\n",
    "    x=tweet._json\n",
    "    if len(x)!=0:\n",
    "        break\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Picking only the necesssary information from the JSON files and storing them in a list for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 824\n",
      "Rate limit reached. Sleeping for: 820\n"
     ]
    }
   ],
   "source": [
    "columns = ['User', 'Tweet','Date','location']\n",
    "data = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    data.append([tweet.full_text])\n",
    "\n",
    "# Russia_df = pd.DataFrame(data, columns=columns)\n",
    "# Russia_df['label']='Russia'\n",
    "# Russia_df.to_csv('Russia_50,000.csv')\n",
    "# print(len(Russia_df))\n",
    "# Russia_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeating the above process again for tweets with the Ukraine hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 821\n",
      "Rate limit reached. Sleeping for: 820\n",
      "Rate limit reached. Sleeping for: 822\n"
     ]
    }
   ],
   "source": [
    "keywords=\"#Ukraine since:\" + str(yesterday)+ \" until:\" + str(today)\n",
    "limit=50000\n",
    "tweets = tweepy.Cursor(api.search_tweets,since_id=\"2022-01-01\", q=keywords, count=300, tweet_mode='extended',lang=\"en\").items(limit)\n",
    "\n",
    "#######################################################################\n",
    "# columns = ['User', 'Tweet','Date','location']\n",
    "data_1 = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    data_1.append([tweet.full_text])\n",
    "\n",
    "# Ukraine_df = pd.DataFrame(data_1, columns=columns)\n",
    "# Ukraine_df['label']='Ukraine'\n",
    "# Ukraine_df.to_csv('Ukraine_50,000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the created datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shzso\\AppData\\Local\\Temp\\ipykernel_27148\\2503116619.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df=df_Russia.append(df_Ukraine , ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df_Russia=pd.read_csv(r'Russia_50,000.csv')\n",
    "df_Ukraine=pd.read_csv(r'Ukraine_50,000.csv')\n",
    "df=df_Russia.append(df_Ukraine , ignore_index=True)\n",
    "df = df.sample(frac=1,random_state=218).reset_index(drop=True)\n",
    "df['label']=df['label'].where(df['label']=='Russia',0)\n",
    "df['label']=df['label'].where(df['label']==0,1)\n",
    "# df.isna().sum()\n",
    "# df_Ukraine\n",
    "df=df.Tweet\n",
    "df.head()\n",
    "testset=df[:50]\n",
    "testset_labels=np.array([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
    "       1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 0, 0, 0, 1, 0])\n",
    "df=df[50:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs=[]\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for i in range(len(df)):\n",
    "    temp=(analyzer.polarity_scores(df[i]))\n",
    "    vs.append(temp['compound'])\n",
    "\n",
    "for i in range(len(vs)):\n",
    "    if vs[i]>=0:\n",
    "        vs[i]=1\n",
    "    if vs[i] < 0:\n",
    "        vs[i]=0\n",
    "# df[0]\n",
    "# temp=analyzer.polarity_scores(df[2])\n",
    "vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences_unprocessed, testing_sentences_unprocessed, training_labels, testing_labels=train_test_split(df, vs, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the tweets\n",
    "def process_tweet(tweet):\n",
    "    vocab_size = 10000\n",
    "    oov_tok = '<OOV>'\n",
    " \n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove twitter abbreviations\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = []\n",
    "for i, tweet in enumerate(training_sentences_unprocessed): \n",
    "    training_sentences.append(process_tweet(tweet))\n",
    "\n",
    "testing_sentences = []\n",
    "for i, tweet in enumerate(testing_sentences_unprocessed): \n",
    "    testing_sentences.append(process_tweet(tweet))\n",
    "\n",
    "# Convert labels lists to numpy array\n",
    "training_labels_final = []\n",
    "testing_labels_final = []\n",
    "\n",
    "for i in training_labels:\n",
    "    training_labels_final.append(tf.cast(i, tf.int32))\n",
    "\n",
    "for i in testing_labels:\n",
    "    testing_labels_final.append(tf.cast(i, tf.int32))\n",
    "\n",
    "\n",
    "training_labels_final = np.array(training_labels_final)\n",
    "testing_labels_final = np.array(testing_labels_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "vocab_size = 7000\n",
    "max_length = 120\n",
    "embedding_dim = 16\n",
    "trunc_type='post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "\n",
    "# Generate the word index dictionary for the training sentences\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Generate and pad the training sequences\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "# Generate and pad the test sequences\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 120, 16)           112000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 120)               65760     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 120)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 726       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 178,493\n",
      "Trainable params: 178,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.LSTM(120),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Setup the training parameters\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2093/2093 [==============================] - 32s 14ms/step - loss: 0.2310 - accuracy: 0.8989 - val_loss: 0.1460 - val_accuracy: 0.9444\n",
      "Epoch 2/5\n",
      "2093/2093 [==============================] - 29s 14ms/step - loss: 0.1105 - accuracy: 0.9581 - val_loss: 0.1223 - val_accuracy: 0.9543\n",
      "Epoch 3/5\n",
      "2093/2093 [==============================] - 29s 14ms/step - loss: 0.0796 - accuracy: 0.9699 - val_loss: 0.1266 - val_accuracy: 0.9569\n",
      "Epoch 4/5\n",
      "2093/2093 [==============================] - 33s 16ms/step - loss: 0.0587 - accuracy: 0.9774 - val_loss: 0.1286 - val_accuracy: 0.9577\n",
      "Epoch 5/5\n",
      "2093/2093 [==============================] - 35s 17ms/step - loss: 0.0450 - accuracy: 0.9826 - val_loss: 0.1414 - val_accuracy: 0.9579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: senitment_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: senitment_model\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002CED93C7490> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))\n",
    "model.save('senitment_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Get the embedding layer from the model (i.e. first layer)\n",
    "embedding_layer = model.layers[0]\n",
    "\n",
    "# Get the weights of the embedding layer\n",
    "embedding_weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "# Print the shape. Expected is (vocab_size, embedding_dim)\n",
    "print(embedding_weights.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index-word dictionary\n",
    "reverse_word_index = tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Open writeable files\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "# Initialize the loop. Start counting at `1` because `0` is just for the padding\n",
    "for word_num in range(1, vocab_size):\n",
    "\n",
    "  # Get the word associated at the current index\n",
    "  word_name = reverse_word_index[word_num]\n",
    "\n",
    "  # Get the embedding weights associated with the current index\n",
    "  word_embedding = embedding_weights[word_num]\n",
    "\n",
    "  # Write the word name\n",
    "  out_m.write(word_name + \"\\n\")\n",
    "\n",
    "  # Write the word embedding\n",
    "  out_v.write('\\t'.join([str(x) for x in word_embedding]) + \"\\n\")\n",
    "\n",
    "# Close the files\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1333\"\n",
       "            height=\"900\"\n",
       "            src=\"https://projector.tensorflow.org/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2cefa99ca30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://projector.tensorflow.org/'\n",
    "\n",
    "IPython.display.IFrame(url, width=1333, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final testing to see which model performed better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores (testset,testset_labels):\n",
    "    final_test_sentences = []\n",
    "    for i, tweet in enumerate(testset): \n",
    "        final_test_sentences.append(process_tweet(tweet[0]))\n",
    "\n",
    "\n",
    "    testing_sequences = tokenizer.texts_to_sequences(final_test_sentences)\n",
    "    final_test_padded = pad_sequences(testing_sequences,maxlen=max_length)\n",
    "\n",
    "    prediction=(model.predict(final_test_padded).round()).flatten()\n",
    "    prediction=prediction.astype(int)\n",
    "\n",
    "    vs_test=[]\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for i in range(50):\n",
    "        temp=(analyzer.polarity_scores(testset.iloc[i]))\n",
    "        vs_test.append(temp['compound'])\n",
    "\n",
    "    for i in range(len(vs_test)):\n",
    "        if vs_test[i]>=0:\n",
    "            vs_test[i]=1\n",
    "        if vs_test[i] < 0:\n",
    "            vs_test[i]=0\n",
    "    vs_test=np.array(vs_test)\n",
    "\n",
    "    # pred_accuracy=((prediction==testset_labels))#.sum()/700)*100\n",
    "    # vs_accuracy=((vs_test==testset_labels))#.sum()/700)*100\n",
    "    return prediction,vs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural network acc: 0.66 vader sentiment accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prediction, vs_test=get_scores(testset=testset,testset_labels=testing_labels)\n",
    "model_prediction_acc=(testset_labels==prediction).sum()/len(testset_labels)\n",
    "vader_prediction_acc=(testset_labels==vs_test).sum()/len(testset_labels)\n",
    "print(f'neural network acc: {model_prediction_acc} vader sentiment accuracy: {vader_prediction_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "+ As we can see from above we get a higher accuracy from the LSTM model and a lower accuracy from the simpler model\n",
    "+ one can assume the difference in accuracy can be seen because our model does not assume the independance of words and instead treats the word in the context of the sentence.\n",
    "+ Although there is an increase in accufracy this cannot be treated as conclusive evidence for this and we need to perform further tests over different models to reach this conclusion\n",
    "+ For th emeantime the call to action for twitter is to create a deep learning neural network to suggest a sentiment analysis for its users instead of relying on the simpler model/]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34e38b871a78ce03784bee49b7081b844d23d64c1ab896e81272e7d6c67059ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
